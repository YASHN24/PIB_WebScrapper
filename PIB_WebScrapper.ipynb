{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs19p2AWiPXo"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 pandas\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to get the article details from the press release page\n",
        "def get_article_details(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the date of publication\n",
        "    date_tag = soup.find('div', class_='ReleaseDateSubHeaddateTime text-center pt20')\n",
        "    date_of_publication = date_tag.text.strip() if date_tag else 'N/A'\n",
        "\n",
        "    # Extract the headline\n",
        "    headline_tag = soup.find('div', class_='innner-page-main-about-us-content-right-part').find('h2')\n",
        "    headline = headline_tag.text.strip() if headline_tag else 'N/A'\n",
        "\n",
        "    # Extract the ministry\n",
        "    ministry_tag = soup.find('div', class_='MinistryNameSubhead text-center')\n",
        "    ministry = ministry_tag.text.strip() if ministry_tag else 'N/A'\n",
        "\n",
        "    # Extract the article content\n",
        "    content_tag = soup.find('div', class_='innner-page-main-about-us-content-right-part')\n",
        "    paragraphs = content_tag.find_all('p') if content_tag else []\n",
        "    article_content = ' '.join([para.get_text(separator=' ', strip=True) for para in paragraphs])\n",
        "\n",
        "    return date_of_publication, headline, ministry, article_content, url\n",
        "\n",
        "# Function to generate the list of URLs based on the range of PRIDs\n",
        "def generate_urls(start_prid, end_prid):\n",
        "    base_url = \"https://pib.gov.in/PressReleasePage.aspx?PRID=\"\n",
        "    prids = range(start_prid, end_prid + 1)\n",
        "    urls = [f\"{base_url}{prid}\" for prid in prids]\n",
        "    return urls\n",
        "\n",
        "# Main function to scrape the data and write to a CSV file\n",
        "def scrape_pib_data(start_prid, end_prid, output_file):\n",
        "    urls = generate_urls(start_prid, end_prid)\n",
        "\n",
        "    # Open a CSV file to write the data\n",
        "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Date of Publication', 'Headline', 'Ministry', 'Article Content', 'Article Link'])\n",
        "\n",
        "        # Iterate through each URL and get the details\n",
        "        for url in urls:\n",
        "            try:\n",
        "                date_of_publication, headline, ministry, article_content, article_link = get_article_details(url)\n",
        "                writer.writerow([date_of_publication, headline, ministry, article_content, article_link])\n",
        "                print(f\"Scraped data from {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to scrape {url}: {e}\")\n",
        "\n",
        "# Specify the range of PRIDs and the output CSV file\n",
        "start_prid = 1992019\n",
        "end_prid = 2016839\n",
        "output_file = 'pib_data.csv'\n",
        "# Call the main function to start scraping\n",
        "scrape_pib_data(start_prid, end_prid, output_file)\n"
      ]
    }
  ]
}